{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Road Event Video Search: Computer Vision Notebook\n",
        "\n",
        "In this notebook, we will implement our system to perform query search on a video using the ViT (Visual Transformer) model from Clip. This model process text and image into embeddings, and then compared them to find the frame in video corresponding to the text query.\n",
        "\n",
        "The processes in this notebook are:\n",
        "1. Install Libraries\n",
        "2. Import Libraries and Load Model\n",
        "3. Inference Function\n",
        "4. Run and Deploy Inference Interface using Google Colab"
      ],
      "metadata": {
        "id": "RvJ-FJmQr-f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Libraries"
      ],
      "metadata": {
        "id": "RAWVWJWes67B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT6QbsSx9Eo_",
        "outputId": "be7d2c13-98ba-49c9-f586-5d96042e78f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.7/dist-packages (3.10.1)\n",
            "Requirement already satisfied: markdown-it-py[linkify,plugins] in /usr/local/lib/python3.7/dist-packages (from gradio) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: paramiko in /usr/local/lib/python3.7/dist-packages (from gradio) (2.12.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from gradio) (3.15.0)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.7/dist-packages (from gradio) (10.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.7/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.7/dist-packages (from gradio) (0.19.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.7/dist-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.3)\n",
            "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.7/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (from gradio) (0.87.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.7/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.10.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Requirement already satisfied: starlette==0.21.0 in /usr/local/lib/python3.7/dist-packages (from fastapi->gradio) (0.21.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.21.0->fastapi->gradio) (3.6.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.21.0->fastapi->gradio) (1.3.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (1.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.2)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.1)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.7/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.6)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (38.0.3)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (4.0.1)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2zrv77ht\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-2zrv77ht\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OCePqneRxkoRSEunEJ45QMRXkhhPjj8O\n",
            "To: /content/image-not-found-scaled.png\n",
            "100% 223k/223k [00:00<00:00, 106MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install ftfy\n",
        "!pip install regex \n",
        "!pip install tqdm\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1OCePqneRxkoRSEunEJ45QMRXkhhPjj8O"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Libraries and Load Model"
      ],
      "metadata": {
        "id": "lHnNjC9Ns_0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "81fxNbHO86VK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.system(\"pip freeze\")\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import clip\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import datetime\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# Query choices\n",
        "choices_to_query = {\n",
        "    \"Crash\": \"vehicle crash\",\n",
        "    \"Traffic Jam\": \"traffic jam\",\n",
        "    \"Flood\": \"flash flood\",\n",
        "    \"Demonstration\": \"crowd demonstration\"\n",
        "}\n",
        "\n",
        "threshold_dict = {\n",
        "    \"Crash\": 29,\n",
        "    \"Traffic Jam\": 25,\n",
        "    \"Flood\": 25,\n",
        "    \"Demonstration\": 23\n",
        "}\n",
        "\n",
        "blank_image = Image.open('/content/image-not-found-scaled.png')\n",
        "\n",
        "# Load the open CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Inference Function"
      ],
      "metadata": {
        "id": "Lt4pOf-8tJcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "unZEelD1OYIB"
      },
      "outputs": [],
      "source": [
        "def inference(video, query, advance_query, slider):\n",
        "  # The frame images will be stored in video_frames\n",
        "  video_frames = []\n",
        "\n",
        "  # Open the video file\n",
        "  capture = cv2.VideoCapture(video)\n",
        "  fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "  total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  \n",
        "  current_frame = 1\n",
        "  num_skip_frames = slider\n",
        "  start = time.time()\n",
        "  while capture.isOpened():\n",
        "    if current_frame <= num_skip_frames:\n",
        "      ret = capture.grab()\n",
        "      current_frame += 1\n",
        "      continue\n",
        "    ret, frame = capture.read()\n",
        "    current_frame = 1\n",
        "    # print('Read a new frame: ', ret)\n",
        "    if ret:\n",
        "      video_frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  # Print some statistics\n",
        "  print(f\"Finished extracting frames in {time.time()-start} s\")\n",
        "  print(f\"Frames extracted: {len(video_frames)}\")\n",
        "  \n",
        "  # You can try tuning the batch size for very large videos, but it should usually be OK\n",
        "  batch_size = 256\n",
        "  batches = math.ceil(len(video_frames) / batch_size)\n",
        "  \n",
        "  # The encoded features will bs stored in video_features\n",
        "  video_features = []\n",
        "  \n",
        "  # Process each batch\n",
        "  for i in range(batches):\n",
        "    print(f\"Processing batch {i+1}/{batches}\")\n",
        "  \n",
        "    # Get the relevant frames\n",
        "    batch_frames = video_frames[i*batch_size : (i+1)*batch_size]\n",
        "    \n",
        "    # Preprocess the images for the batch\n",
        "    batch_preprocessed = torch.stack([preprocess(frame) for frame in batch_frames]).to(device)\n",
        "    \n",
        "    # Encode with CLIP and normalize\n",
        "    with torch.no_grad():\n",
        "      batch_features = model.encode_image(batch_preprocessed)\n",
        "      batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
        "  \n",
        "    # Append the batch to the list containing all features\n",
        "    video_features.append(batch_features)\n",
        "\n",
        "  video_features = torch.cat(video_features, dim=0)\n",
        "  \n",
        "  # Print some stats\n",
        "  print(f\"Features: {video_features.shape}\")\n",
        " \n",
        "  search_query=query\n",
        "  display_heatmap=False\n",
        "  display_results_count=1\n",
        "\n",
        "  # Encode and normalize the search query using CLIP\n",
        "  if not isinstance(search_query, str):\n",
        "    query_input = preprocess(Image.fromarray(search_query)).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "      query_features = model.encode_image(query_input)\n",
        "      query_features /= query_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  else:\n",
        "    if advance_query != \"\":\n",
        "      search_query = advance_query\n",
        "    else:\n",
        "      search_query = choices_to_query[search_query]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      query_features = model.encode_text(clip.tokenize(search_query).to(device))\n",
        "      query_features /= query_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # Compute the similarity between the search query and each frame using the Cosine similarity\n",
        "  similarities = (100.0 * video_features @ query_features.t())\n",
        "  print(similarities.cpu().tolist())\n",
        "  values, best_photo_idx = similarities.topk(display_results_count, dim=0)\n",
        "\n",
        "  for frame_id in best_photo_idx:\n",
        "    frame_id = frame_id.item()\n",
        "    frame = video_frames[frame_id]\n",
        "    # Find the timestamp in the video and display it\n",
        "    frame_id += (frame_id+1)*num_skip_frames\n",
        "    seconds = round(frame_id/fps)\n",
        "\n",
        "  if query in threshold_dict:\n",
        "    threshold = threshold_dict[query]\n",
        "    if values.item() < threshold:\n",
        "      return blank_image, \"There is no frame corresponding to the query\"\n",
        "\n",
        "  return frame, f\"Found at {str(datetime.timedelta(seconds=seconds))} with similarity {values.item()}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run and Deploy Inference Interface Using Google Colab\n"
      ],
      "metadata": {
        "id": "BPqId2BItNs5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "P70SJmaUUwDj",
        "outputId": "f5756cbb-c05d-4070-e803-6d948befb0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:43: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://e12648f1ef7b10e5.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e12648f1ef7b10e5.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "  gr.Markdown(f\"\"\"\n",
        "  # Road Event Video Search\n",
        "  To use this interactive demo:\n",
        "  1. Upload a video you want to query\n",
        "  2. Select text query\n",
        "  3. Click **Search**.\n",
        "  \"\"\")\n",
        "  choices = list(choices_to_query.keys())\n",
        "  with gr.Tab(\"Text Query\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        text_video_comp = gr.Video(label=\"Input Video\")\n",
        "        simple_text_query_comp = gr.Radio(choices=choices, value=choices[0], label=\"Text Query\")\n",
        "        with gr.Accordion(\"Advance Query\", open=False):\n",
        "          advance_query = gr.Textbox(label=\"Advance Text Query\")\n",
        "        text_num_skip_frames_comp = gr.Slider(0, 100, value=0, step=1, label=\"Num Skip Frames\")\n",
        "        text_button = gr.Button(\"Search\")\n",
        "      with gr.Column():\n",
        "        text_output = [gr.outputs.Image(type=\"pil\", label=\"Output\"), gr.Textbox(label=\"Output\")]\n",
        "  with gr.Tab(\"Image Query\"):\n",
        "    with gr.Row():\n",
        "      with gr.Column():\n",
        "        img_video_comp = gr.Video(label=\"Input Video\")\n",
        "        image_query_comp = gr.Image(label=\"Image Query\")\n",
        "        img_num_skip_frames_comp = gr.Slider(0, 100, value=0, step=1, label=\"Num Skip Frames\")\n",
        "        image_button = gr.Button(\"Search\")\n",
        "      with gr.Column():\n",
        "        image_output = [gr.outputs.Image(type=\"pil\", label=\"Output\"), gr.Textbox(label=\"Output\")]\n",
        "\n",
        "  text_button.click(inference, inputs=[text_video_comp, simple_text_query_comp, advance_query, text_num_skip_frames_comp], outputs=text_output)\n",
        "  image_button.click(inference, inputs=[img_video_comp, image_query_comp, advance_query, img_num_skip_frames_comp], outputs=image_output)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}